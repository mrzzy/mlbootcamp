{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 6 - Reviews Sentiment Classification \n",
    "In this notebook, we will attempt to classify, whether a review is good or bad, given the text of the review.  \n",
    "This is the solution notebook for the [Kaggle Competition Here](https://www.kaggle.com/c/mixed-reviews-dataset/overview)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "from keras import (models, optimizers, layers, callbacks, \n",
    "                   regularizers)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import baseline_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcing the data\n",
    "We will use pandas to source the data from the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(\"reviews_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the test data from the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"reviews_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "Taking a look at the first five rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>102 Dalmatians (2000, Dir. Kevin Lima) &lt;br /&gt;&lt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>By 1971 it was becoming more and more obvious ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This film, once sensational for its forward-th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>One has to be careful whom one tells about wat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I read somewhere where this film was supposed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  label\n",
       "0           0  102 Dalmatians (2000, Dir. Kevin Lima) <br /><...      0\n",
       "1           1  By 1971 it was becoming more and more obvious ...      0\n",
       "2           2  This film, once sensational for its forward-th...      0\n",
       "3           3  One has to be careful whom one tells about wat...      1\n",
       "4           4  I read somewhere where this film was supposed ...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Unnamed: 0` column is confusingly named, upon closer inspection, we realise that it is actually the `id`.\n",
    "\n",
    "As such we rename `Unnamed: 0` to `id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'review', 'label'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = reviews_df.rename(columns={\"Unnamed: 0\": \"id\"})\n",
    "test_df = test_df.rename(columns={\"Unnamed: 0\": \"id\"})\n",
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at a random review to see how the dataset works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      " I bought this movie last weekend at my local Movie Gallery. It was buy 2 get 2 free and I needed one more so I chose this one. Horrible mistake. The box reads like it would be a really good movie. Well, it starts out like it is going to be this great movie. For about 5 minutes, that is. The movie is about a young woman, Laila, who gets killed trying to save her beau, Jack, from a bull. Laila's dad, Cordobes, is a rancher that the townspeople are afraid of. He assumes that Jack killed Laila because she was supposedly afraid of this bull, and goes on this hunt to find him. That was the first 5 minutes that is good. What follows after that is only gonna get 100 times worse. Whoever wrote the script, in my opinion, had to of been on some kind acid trip or something because nothing else made any sense what so ever. Jack is on the run and finds this traveling radio DJ named Mary who gives him a ride. I think Mary is supposed to be a virgin Mary type character. You know, Jesus' mother. But, who knows, I couldn't make heads or tails of it. As they're running... we get to see bad guys, magical visions, ghostly encounters, flashbacks, etc... And all these things are done in such a way that your brain hurts from trying to figure out what's going on. Needless to say, I took the movie back and exchanged it for something else. It's horrible I tell ya, horrible. And, there is absolutely no bull-fighting in this movie. Unless you count the first minute of the movie. Hope I helped some other people keep from wasting their time on this movie. \n",
      " Label:  0\n"
     ]
    }
   ],
   "source": [
    "rand_idx = random.randint(0, len(reviews_df))\n",
    "# Get random review and label from reviews dataframe \n",
    "rand_review = reviews_df.loc[rand_idx, \"review\"]\n",
    "rand_label = reviews_df.loc[rand_idx, \"label\"]\n",
    "\n",
    "print(\"Review: \\n\", rand_review, \"\\n Label: \", rand_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe from the reviews, we have to do the following steps before we can conduct ML:\n",
    "- Split up the words in the review\n",
    "- Remove the HTML tags (ie `<br/>`)\n",
    "- Deal with the punctuation somehow>\n",
    "- Dealing capital letters in the reviews\n",
    "- Dealing with reviews with no words whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "Removing the HTML tags from the data as they not relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy so that we dont overwrite the original dataframe\n",
    "pp_reviews_df = reviews_df.copy()\n",
    "pp_test_df = test_df.copy()\n",
    "# remove html tags\n",
    "pp_reviews_df[\"review\"] = reviews_df[\"review\"].transform(\n",
    "    (lambda review: re.sub(\"<.+>\", \"\", review)))\n",
    "pp_test_df[\"review\"] = test_df[\"review\"].transform(\n",
    "    (lambda review: re.sub(\"<.+>\", \"\", review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_str):\n",
    "    for p in string.punctuation:\n",
    "        input_str = input_str.replace(p, \"\")\n",
    "    return input_str\n",
    "\n",
    "pp_reviews_df[\"review\"] = pp_reviews_df[\"review\"].transform(\n",
    "     remove_punctuation)\n",
    "pp_test_df[\"review\"] = pp_test_df[\"review\"].transform(\n",
    "     remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the captial letters by converting them to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_reviews_df[\"review\"] = pp_reviews_df[\"review\"].transform(\n",
    "    (lambda review: review.lower()))\n",
    "pp_test_df[\"review\"] = pp_test_df[\"review\"].transform(\n",
    "    (lambda review: review.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished with the data cleaning process.  \n",
    "Lets take a look at the processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>102 dalmatians 2000 dir kevin lima shes change...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>by 1971 it was becoming more and more obvious ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this film once sensational for its forwardthin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>one has to be careful whom one tells about wat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i read somewhere where this film was supposed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             review  label\n",
       "0   0  102 dalmatians 2000 dir kevin lima shes change...      0\n",
       "1   1  by 1971 it was becoming more and more obvious ...      0\n",
       "2   2  this film once sensational for its forwardthin...      0\n",
       "3   3  one has to be careful whom one tells about wat...      1\n",
       "4   4  i read somewhere where this film was supposed ...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Words into Vectors\n",
    "#### One hot encoding\n",
    "We will convert the each review into a vector of counts of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "reviews = pp_reviews_df[\"review\"]\n",
    "review_vectors = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0b029ef54a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreview_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "review_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "required memory:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6945550000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"required memory:\")\n",
    "len(pp_reviews_df) * len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, One Hot Encoding the data using `CountVectorizer` does not seem very feasible because it requires a load of RAM we do not have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction using Word Vectors\n",
    "Here we apply a form of transfer learning: using other peoples models to do feature extraction. In this case we use Facebook AI research's Fasttext model to convert the words to vectors\n",
    "\n",
    "First we need to download [Fasttext pretrained word vectors](https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip) and  unzip it in the notebook's folder\n",
    "\n",
    "Then we load the word to vector encoder like so:\n",
    "> note: its going to take some time to load the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzy/.conda/envs/mlbootcamp/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 334 ms, total: 1min 9s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoder = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\",\n",
    "                                            limit=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the vectors, we can convert the words in the \n",
    "reviews into word vectors.\n",
    "\n",
    "Since vector arithmetic works on word vectors, we compute the mean of the word vectors for each review to get a vector representing the \"meaning\" of review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 s, sys: 35.8 ms, total: 16.9 s\n",
      "Wall time: 16.9 s\n",
      "CPU times: user 1.02 s, sys: 11 Âµs, total: 1.02 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "# define a function to convert a review into a vector\n",
    "def convert_review_to_vector(review):\n",
    "    # split the review into words\n",
    "    words = review.split()\n",
    "    \n",
    "    # convert to the words into vectors\n",
    "    unknown_vector = np.zeros((300,))\n",
    "    vectors = [ unknown_vector ] # handle no word reviews\n",
    "    \n",
    "    for word in words:\n",
    "        if word in encoder.vocab:\n",
    "            vector = encoder[word]\n",
    "        else: # word is not in encoder vocabulary\n",
    "            vector = unknown_vector\n",
    "        vectors.append(vector) # convert to float32 to reduce ram use\n",
    "\n",
    "    # mean up the word vectors to get the \"meaning\" of the review\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# convert the reviews in the dataframe itself\n",
    "%time pp_reviews_df[\"review\"] = pp_reviews_df[\"review\"].transform(convert_review_to_vector)\n",
    "%time pp_test_df[\"review\"] = pp_test_df[\"review\"].transform(convert_review_to_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract out the inputs (review vectors) and the outputs (labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vectors = np.stack(pp_reviews_df[\"review\"].values) #inputs\n",
    "test_review_vectors = np.stack(pp_test_df[\"review\"].values) #inputs\n",
    "labels = np.stack(pp_reviews_df[\"label\"].values) #outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, valid_vectors, train_labels, valid_labels = (\n",
    "    train_test_split(review_vectors, labels, \n",
    "                     test_size=10000,\n",
    "                     shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "Now we proceed to build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model given parameters\n",
    "def build_model(input_shape, n_outputs, scale_width, scale_depth,\n",
    "               activation, l2_lambda):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.InputLayer(input_shape))\n",
    "    \n",
    "    # add hidden layers\n",
    "    for i in range(scale_depth):\n",
    "        model.add(layers.Dense(scale_width,\n",
    "                               kernel_regularizer=\n",
    "                               regularizers.l2(l2_lambda)))\n",
    "        model.add(activation())\n",
    "    \n",
    "    # output layer\n",
    "    model.add(layers.Dense(n_outputs, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our build function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzy/.conda/envs/mlbootcamp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                19264     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 27,649\n",
      "Trainable params: 27,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(input_shape=(300,),\n",
    "                   n_outputs=1,\n",
    "                   scale_width=64,\n",
    "                   scale_depth=3,\n",
    "                   activation=layers.ReLU,\n",
    "                   l2_lambda=0)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Since this is a binary classification problem ('good' or 'bad'),  \n",
    "we should use the `binary_crossentropy` loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzy/.conda/envs/mlbootcamp/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 0.4746 - acc: 0.7711 - val_loss: 0.4188 - val_acc: 0.8090\n",
      "Epoch 2/4\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.4106 - acc: 0.8148 - val_loss: 0.4049 - val_acc: 0.8174\n",
      "Epoch 3/4\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.4025 - acc: 0.8173 - val_loss: 0.4111 - val_acc: 0.8102\n",
      "Epoch 4/4\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3952 - acc: 0.8207 - val_loss: 0.3953 - val_acc: 0.8201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7b3244dd8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We compile the model as follows:\n",
    "model.compile(\n",
    "    optimizers.Adam(lr=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# derive a unique name for our run based on the current time\n",
    "run_name = \"run_\" + datetime.strftime(datetime.now(), \"%Y_%m_%d__%H_%M_%S\")\n",
    "run_path = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "# Training the model:\n",
    "model.fit(train_vectors, train_labels,\n",
    "         validation_data=(valid_vectors, valid_labels),\n",
    "         batch_size=64,\n",
    "         epochs=4,\n",
    "         callbacks=[callbacks.TensorBoard(log_dir=run_path)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating the Model\n",
    "We change different hyperparameters of the model to attempt to improve its performance (accuracy):\n",
    "- learning rate\n",
    "- no. of epochs trained\n",
    "- no. of hidden layers\n",
    "- no. of hidden units\n",
    "- adding regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.4902 - acc: 0.7801 - val_loss: 0.4610 - val_acc: 0.7995\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4399 - acc: 0.8125 - val_loss: 0.4326 - val_acc: 0.8152\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4288 - acc: 0.8179 - val_loss: 0.4264 - val_acc: 0.8155\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4248 - acc: 0.8163 - val_loss: 0.4259 - val_acc: 0.8151\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4199 - acc: 0.8188 - val_loss: 0.4327 - val_acc: 0.8124\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4190 - acc: 0.8198 - val_loss: 0.4273 - val_acc: 0.8123\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4135 - acc: 0.8233 - val_loss: 0.4183 - val_acc: 0.8183\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 1s 28us/step - loss: 0.4132 - acc: 0.8221 - val_loss: 0.4150 - val_acc: 0.8222\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.4097 - acc: 0.8240 - val_loss: 0.4247 - val_acc: 0.8160\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.4090 - acc: 0.8233 - val_loss: 0.4156 - val_acc: 0.8222\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.4092 - acc: 0.8245 - val_loss: 0.4204 - val_acc: 0.8166\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4071 - acc: 0.8247 - val_loss: 0.4130 - val_acc: 0.8198\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 1s 28us/step - loss: 0.4054 - acc: 0.8249 - val_loss: 0.4099 - val_acc: 0.8229\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4045 - acc: 0.8267 - val_loss: 0.4131 - val_acc: 0.8214\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4015 - acc: 0.8264 - val_loss: 0.4083 - val_acc: 0.8228\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.4007 - acc: 0.8287 - val_loss: 0.4064 - val_acc: 0.8266\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3999 - acc: 0.8276 - val_loss: 0.4166 - val_acc: 0.8210\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3991 - acc: 0.8286 - val_loss: 0.4190 - val_acc: 0.8151\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3973 - acc: 0.8289 - val_loss: 0.4255 - val_acc: 0.8113\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3961 - acc: 0.8297 - val_loss: 0.4134 - val_acc: 0.8198\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3885 - acc: 0.8341 - val_loss: 0.4107 - val_acc: 0.8242\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3869 - acc: 0.8360 - val_loss: 0.4087 - val_acc: 0.8205\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3863 - acc: 0.8344 - val_loss: 0.4054 - val_acc: 0.8268\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3870 - acc: 0.8339 - val_loss: 0.4231 - val_acc: 0.8152\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3833 - acc: 0.8370 - val_loss: 0.4042 - val_acc: 0.8262\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3834 - acc: 0.8355 - val_loss: 0.4041 - val_acc: 0.8249\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3827 - acc: 0.8374 - val_loss: 0.4056 - val_acc: 0.8263\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3815 - acc: 0.8377 - val_loss: 0.4059 - val_acc: 0.8217\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3810 - acc: 0.8381 - val_loss: 0.4058 - val_acc: 0.8238\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3783 - acc: 0.8395 - val_loss: 0.4040 - val_acc: 0.8262\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3800 - acc: 0.8367 - val_loss: 0.4048 - val_acc: 0.8238\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3801 - acc: 0.8377 - val_loss: 0.4023 - val_acc: 0.8254\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3764 - acc: 0.8395 - val_loss: 0.4010 - val_acc: 0.8268\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3775 - acc: 0.8387 - val_loss: 0.4067 - val_acc: 0.8221\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3759 - acc: 0.8395 - val_loss: 0.4079 - val_acc: 0.8232\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3735 - acc: 0.8413 - val_loss: 0.4036 - val_acc: 0.8256\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3754 - acc: 0.8388 - val_loss: 0.4089 - val_acc: 0.8237\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3681 - acc: 0.8435 - val_loss: 0.4022 - val_acc: 0.8265\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3687 - acc: 0.8430 - val_loss: 0.4025 - val_acc: 0.8272\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3683 - acc: 0.8444 - val_loss: 0.4093 - val_acc: 0.8231\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3657 - acc: 0.8455 - val_loss: 0.4015 - val_acc: 0.8269\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3660 - acc: 0.8462 - val_loss: 0.4009 - val_acc: 0.8268\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3657 - acc: 0.8451 - val_loss: 0.4088 - val_acc: 0.8249\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3639 - acc: 0.8468 - val_loss: 0.4043 - val_acc: 0.8276\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3646 - acc: 0.8469 - val_loss: 0.4114 - val_acc: 0.8245\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3636 - acc: 0.8475 - val_loss: 0.4068 - val_acc: 0.8240\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3594 - acc: 0.8498 - val_loss: 0.4015 - val_acc: 0.8261\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3591 - acc: 0.8500 - val_loss: 0.3999 - val_acc: 0.8275\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3592 - acc: 0.8495 - val_loss: 0.4007 - val_acc: 0.8271\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3579 - acc: 0.8505 - val_loss: 0.4054 - val_acc: 0.8252\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.3582 - acc: 0.8504 - val_loss: 0.4007 - val_acc: 0.8291\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3566 - acc: 0.8511 - val_loss: 0.4114 - val_acc: 0.8254\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3567 - acc: 0.8514 - val_loss: 0.4022 - val_acc: 0.8281\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3542 - acc: 0.8529 - val_loss: 0.4024 - val_acc: 0.8278\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3540 - acc: 0.8524 - val_loss: 0.4013 - val_acc: 0.8274\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3535 - acc: 0.8529 - val_loss: 0.4012 - val_acc: 0.8274\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3533 - acc: 0.8531 - val_loss: 0.4011 - val_acc: 0.8271\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3528 - acc: 0.8534 - val_loss: 0.4015 - val_acc: 0.8260\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3523 - acc: 0.8534 - val_loss: 0.4025 - val_acc: 0.8283\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3527 - acc: 0.8525 - val_loss: 0.4017 - val_acc: 0.8279\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3509 - acc: 0.8544 - val_loss: 0.4014 - val_acc: 0.8275\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3508 - acc: 0.8538 - val_loss: 0.4014 - val_acc: 0.8269\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3508 - acc: 0.8548 - val_loss: 0.4014 - val_acc: 0.8273\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3505 - acc: 0.8543 - val_loss: 0.4011 - val_acc: 0.8273\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3506 - acc: 0.8550 - val_loss: 0.4031 - val_acc: 0.8278\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3500 - acc: 0.8552 - val_loss: 0.4018 - val_acc: 0.8290\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3500 - acc: 0.8551 - val_loss: 0.4011 - val_acc: 0.8269\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3492 - acc: 0.8552 - val_loss: 0.4032 - val_acc: 0.8275\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3491 - acc: 0.8559 - val_loss: 0.4014 - val_acc: 0.8270\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.3490 - acc: 0.8554 - val_loss: 0.4021 - val_acc: 0.8282\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3489 - acc: 0.8558 - val_loss: 0.4014 - val_acc: 0.8266\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3488 - acc: 0.8564 - val_loss: 0.4018 - val_acc: 0.8266\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3487 - acc: 0.8561 - val_loss: 0.4017 - val_acc: 0.8266\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.3487 - acc: 0.8559 - val_loss: 0.4023 - val_acc: 0.8283\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3481 - acc: 0.8562 - val_loss: 0.4015 - val_acc: 0.8270\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3482 - acc: 0.8563 - val_loss: 0.4017 - val_acc: 0.8263\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3482 - acc: 0.8556 - val_loss: 0.4019 - val_acc: 0.8265\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3479 - acc: 0.8565 - val_loss: 0.4019 - val_acc: 0.8270\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 1s 28us/step - loss: 0.3481 - acc: 0.8559 - val_loss: 0.4020 - val_acc: 0.8272\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3479 - acc: 0.8565 - val_loss: 0.4019 - val_acc: 0.8269\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3479 - acc: 0.8561 - val_loss: 0.4018 - val_acc: 0.8261\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3477 - acc: 0.8571 - val_loss: 0.4019 - val_acc: 0.8271\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3477 - acc: 0.8563 - val_loss: 0.4018 - val_acc: 0.8266\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3476 - acc: 0.8567 - val_loss: 0.4017 - val_acc: 0.8274\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3476 - acc: 0.8567 - val_loss: 0.4025 - val_acc: 0.8279\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3476 - acc: 0.8565 - val_loss: 0.4019 - val_acc: 0.8271\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3476 - acc: 0.8565 - val_loss: 0.4020 - val_acc: 0.8273\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3475 - acc: 0.8564 - val_loss: 0.4019 - val_acc: 0.8270\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3475 - acc: 0.8564 - val_loss: 0.4019 - val_acc: 0.8271\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3474 - acc: 0.8565 - val_loss: 0.4018 - val_acc: 0.8272\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3474 - acc: 0.8564 - val_loss: 0.4018 - val_acc: 0.8271\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3474 - acc: 0.8563 - val_loss: 0.4019 - val_acc: 0.8270\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3474 - acc: 0.8566 - val_loss: 0.4020 - val_acc: 0.8273\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3474 - acc: 0.8564 - val_loss: 0.4018 - val_acc: 0.8273\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3474 - acc: 0.8566 - val_loss: 0.4019 - val_acc: 0.8271\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3473 - acc: 0.8565 - val_loss: 0.4019 - val_acc: 0.8270\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3473 - acc: 0.8565 - val_loss: 0.4019 - val_acc: 0.8270\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.3473 - acc: 0.8563 - val_loss: 0.4018 - val_acc: 0.8272\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.3473 - acc: 0.8565 - val_loss: 0.4018 - val_acc: 0.8269\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.3473 - acc: 0.8568 - val_loss: 0.4018 - val_acc: 0.8271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc76056de80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(input_shape=(300,),\n",
    "                   n_outputs=1,\n",
    "                   scale_width=128,\n",
    "                   scale_depth=2,\n",
    "                   activation=(lambda:layers.LeakyReLU(0.3)),\n",
    "                   l2_lambda=1e-4)\n",
    "\n",
    "# We compile the model as follows:\n",
    "model.compile(\n",
    "    optimizers.Adam(lr=1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# derive a unique name for our run based on the current time\n",
    "run_name = \"run_\" + datetime.strftime(datetime.now(), \"%Y_%m_%d__%H_%M_%S\")\n",
    "run_path = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(run_path, exist_ok=True)\n",
    "\n",
    "# Training the model:\n",
    "model.fit(train_vectors, train_labels,\n",
    "         validation_data=(valid_vectors, valid_labels),\n",
    "         batch_size=64,\n",
    "         epochs=100,\n",
    "         callbacks=[callbacks.TensorBoard(log_dir=run_path),\n",
    "                    callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                                               factor=0.5,\n",
    "                                               patience=4,\n",
    "                                               cooldown=4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the Model\n",
    "We predict using the model by feeding it test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probs = model.predict(valid_vectors)\n",
    "test_probs = model.predict(test_review_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We threshold the probabilties to get a set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "valid_preds = np.squeeze(valid_probs >= threshold)\n",
    "test_preds = np.squeeze(test_probs >= threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tune threshold by cross validating using accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8271\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {accuracy_score(valid_labels, valid_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your Predictions\n",
    "Prepare test predictions in the format `id`, `labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   0      1\n",
       "1   1      1\n",
       "2   2      0\n",
       "3   3      0\n",
       "4   4      1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the ids\n",
    "ids = test_df[\"id\"].values\n",
    "\n",
    "# Create the submission dataframe\n",
    "submit_df = pd.DataFrame(data={\n",
    "    \"id\": ids,\n",
    "    \"label\": test_preds.astype(\"int\")\n",
    "})\n",
    "\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write out submission dataframe into a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions: \", len(submit_df))\n",
    "submit_df.to_csv(\"submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbootcamp-py",
   "language": "python",
   "name": "mlbootcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
