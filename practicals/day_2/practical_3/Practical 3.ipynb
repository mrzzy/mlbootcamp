{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Building neural networks with Fashion MNIST\n",
    "In this practical, you will be creating a neural network to identify clothes using the Fashion MNIST dataset.\n",
    "\n",
    "You will be:\n",
    "1. Preparing the dataset\n",
    "2. Creating a neural network\n",
    "3. Training the neural network\n",
    "4. Evaluating the neural network with a learning curve\n",
    "5. Adjusting the model to combat overfitting or underfitting\n",
    "\n",
    "## The Fashion MNIST dataset\n",
    "Link to dataset: https://www.kaggle.com/zalando-research/fashionmnist\n",
    "\n",
    "The Fashion MNIST dataset is a large collection of images of clothes. Each image is made of 28x28 pixels or 784 pixels. The images are black and white, so, there is only one colour channel. Here is what it looks like:\n",
    "\n",
    "![Fashion MNIST images](https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png)\n",
    "\n",
    "There are 10 different fashion items. The dataset labels them as so:\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot\n",
    "\n",
    "Your objective is to build a neural network which is able to classify an image to one of the labels above.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "The first step is to make sure you have data to train and validate your neural network. We will be going through the following steps:\n",
    "\n",
    "1. Downloading the dataset using keras\n",
    "2. Flattening the images from 28x28 to 1x784\n",
    "3. Converting the labels to arrays (one-hot encoding)\n",
    "4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset using keras\n",
    "Since the Fashion MNIST dataset is quite popular, keras allows you to download it just by using an import statement. In the real world, you will probably need to load the dataset from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (val_images, val_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just loaded different sets of data (test, train, images and labels) into a _numpy_ array. Numpy is a mathematics library that helps us deal with maniupulating complex data structures.\n",
    "\n",
    "Let's take a look at the data we just loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning about graphs, you would have learnt about two axes: x and y. Recall also that there is a way to represent a point on the graph by using coordinate system -- you can represent a point on the graph using an x and y value.\n",
    "\n",
    "Extend this idea to programming. Imagine each integer point on the x axis as a bucket which you can a value. For the list `[3, 8, 10, 2]`, the value at\n",
    "\n",
    "| List index | Element | x |\n",
    "| --- | --- | --- |\n",
    "| 0 | 3 | 0 |\n",
    "| 1 | 8 | 1 |\n",
    "| 2 | 10 | 2 |\n",
    "| 3 | 2 | 3 |\n",
    "\n",
    "Now, what if we want a y axis as well? Our list will look like:\n",
    "```python\n",
    "[\n",
    "    [3, 8, 10, 12],\n",
    "    [5, 6, 0, 1],\n",
    "    [5, 8 ,1, 8],\n",
    "]\n",
    "```\n",
    "\n",
    "We call this a 2-dimensional list, because the list now has an additional axis.\n",
    "\n",
    "Back to the code:\n",
    "```python\n",
    "> train_images.shape\n",
    "(60000, 28, 28)\n",
    "```\n",
    "Since there are three values, it means that it is a 3-dimensional list. This is just like the coordinate system. If you have 2 values like (x, y), then you're representing a point in 2-dimensional space.\n",
    "- 60000 represents the number of data points. You can think of them as the number of rows of data.\n",
    "- The following two 28s represent the resolution of the image, 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the images from 28x28 to 784\n",
    "Since our model will not be able to accept a 2 dimensional input (the 28x28 image), we will have to flatten this image to a 1x784 image. We do this using the `.reshape` function. `reshape` changes the shape of the array to whatever we desire, as long as the resulting shape can hold the same number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((train_images.shape[0], 28*28))\n",
    "val_images = val_images.reshape((val_images.shape[0], 28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the labels to arrays (one-hot encoding)\n",
    "The Fashion MNIST labelling system (0-9 for different fashion items) does not work well for neural networks as the neural network will use the number's values as well. For example, when you are assigned your register number in school, let's say 9, it doesn't mean that you are 9th in class or anything like that. It's just a label, a name, an identifier and it should not be interpreted otherwise.\n",
    "\n",
    "To prevent the neural network from interpreting the Fashion MNIST labels as their values, we change the form of the values to something more neutral numerically. We perform what is known as \"one-hot encoding\":\n",
    "\n",
    "| Label | One-hot encoding |\n",
    "| --- | --- |\n",
    "| 0 | `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]` |\n",
    "| 1 | `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]` |\n",
    "| 2 | `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]` |\n",
    "| 3 | `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]` |\n",
    "| 4 | `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]` |\n",
    "| 5 | `[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]` |\n",
    "| 6 | `[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]` |\n",
    "| 7 | `[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]` |\n",
    "| 8 | `[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]` |\n",
    "| 9 | `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]` |\n",
    "\n",
    "The position of the `1` determines the label.\n",
    "\n",
    "Keras provides a function that does this for us: `utils.to_categorical`. We just need to specify how many possible labels there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = utils.to_categorical(train_labels, 10)\n",
    "val_labels = utils.to_categorical(val_labels, 10)\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "As mentioned before, neural networks are really sensitive to big numbers. This applies to the features (which are the inputs) as well. If one feature is bigger than the other, the neural network may think that that feature may be more important. For example, let's say you are trying to predict how likely someone will fall down using two features: height (in cm) and gender (1 for male, 0 for female). Since height is measured in the range of 100s, the neural network may think that it is more important than the gender feature, measured in 1s.\n",
    "\n",
    "To fix this problem, we apply feature scaling. Feature scaling makes all the features have numerically around the same range of numbers. We will be applying standardisation as our method of scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(range(len(train_images)))\n",
    "plt.imshow(train_images[index].reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(images):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(images)\n",
    "    return scaler.transform(images)\n",
    "\n",
    "train_images = scale_features(train_images)\n",
    "val_images = scale_features(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[index].reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a neural network\n",
    "\n",
    "Now we are ready to create the neural network. Recall that there are 3 types of layers: the input layer, hidden layer and output layer.\n",
    "\n",
    "Use dense layers with 600 neurons and ReLU activation for the hidden layers and use a dense layer with 10 neurons (because there are 10 outputs) and softmax activation for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the input layer\n",
    "model.add(layers.InputLayer(input_shape=train_images[0].shape))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "\n",
    "# Add the hidden layers\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "model.add(layers.Dense(600, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(**model_config.compile_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network\n",
    "We will be covering more about things you can configuration for training the neural network in the next lecture. For now, just add `**model_config.train_config` to the end of your parameter list.\n",
    "\n",
    "What you do need to know is how to use the required parameters of the `fit` method. We assign the variable `history` to the results of the model training. `history` records the training statistics for each iteration of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, validation_data=(val_images, val_labels), **model_config.train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the neural network with a learning curve\n",
    "Recall that we can diagnose overfitting and underfitting using two curves: a loss by epochs curve for the training dataset and a loss by epochs curve for the validation dataset.\n",
    "\n",
    "### Underfitting\n",
    "<figure style=\"width: 40%; display: inline-block\">\n",
    "    <img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Example-of-Training-Learning-Curve-Showing-An-Underfit-Model-That-Does-Not-Have-Sufficient-Capacity.png\" alt=\"Underfitting - Model not complex enough\">\n",
    "    <figcaption style=\"text-align: center\">Model not complex enough</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure style=\"width: 40%; display: inline-block\">\n",
    "<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/Example-of-Training-Learning-Curve-Showing-An-Underfit-Model-That-Requires-Further-Training.png\" alt=\"Underfitting - Need more training\">\n",
    "    <figcaption style=\"text-align: center\">Need more training</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Overfitting\n",
    "<figure style=\"width: 40%; display: inline-block\">\n",
    "    <img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-An-Overfit-Model.png\" alt=\"Overfitting\">\n",
    "    <figcaption style=\"text-align: center\">Need more training</figcaption>\n",
    "</figure>\n",
    "\n",
    "So, we need to plot these curves. We can use `matplotlib`'s `pyplot` to plot this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two curves, one for the training loss and one for validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "# Label the graph\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, observing the graph, take the appropriate actions to rectify overfitting or underfitting by changing the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your model in action\n",
    "The following section just tests your model on 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label labels\n",
    "label = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Pick a random image from the validation dataset\n",
    "index = np.random.choice(range(len(val_images)))\n",
    "test_image = val_images[index]\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(test_image.reshape((28, 28)), cmap=\"gray\")\n",
    "print(f\"The test image is a {label[np.argmax(val_labels[index])]}\")\n",
    "\n",
    "# Predict with the model\n",
    "prediction = np.argmax(model.predict(test_image.reshape((1, 784))))\n",
    "\n",
    "print(f\"The model thinks it is a {label[prediction]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
