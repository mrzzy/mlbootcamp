{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 4 - Training Neural Networks\n",
    "In this practical, we will focus on how to effectively train a neural network \n",
    "\n",
    "We will be training a model to classify a small image into 100 different classes/labels.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import datasets\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import losses,metrics\n",
    "from keras import models\n",
    "from keras import callbacks\n",
    "\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import baseline_model\n",
    "import cnn_model\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#%env CUDA_VISIBLE_DEVICES=\"\" # turn off gpu if present\n",
    "tf.set_random_seed(0) # seed rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcing the Data\n",
    "We will be working with the CIFAR 10 dataset: a 10 class/label image classification dataset.  \n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img style=\"display: inline-block\" src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3649/media/cifar-10.png\"/>\n",
    "    <b style=\"display: block\"> Images from CIFAR 10 dataset </b>\n",
    "</div>\n",
    "\n",
    "The task is to classify the 32 by 32 color images into 10 categories.\n",
    "\n",
    "Loading the data is simple as the dataset is built into Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_imgs, train_labels), (valid_imgs, valid_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Lets visualise a random example in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'label: 7')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select a random example in the training set to visualise\n",
    "rand_idx = random.randint(0, len(train_imgs))\n",
    "rand_img = train_imgs[rand_idx]\n",
    "rand_label = train_labels[rand_idx][0]\n",
    "\n",
    "# visualise the image and the corresponding label\n",
    "plt.imshow(rand_img)\n",
    "plt.title(f\"label: {rand_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the label has already been converted to integer for us.  \n",
    "That the data preparation work that we have to do, but a integer is not very interpretable.  \n",
    "We would like to have string label like  'frog' instead of a integer label '6'.\n",
    "To do this we construct a dictionary to map from integer label to string labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metadata for the dataset from disk\n",
    "meta_path = os.path.expanduser(os.path.join(\n",
    "    \"~\", \".keras\", \"datasets\", \"cifar-10-batches-py\", \"batches.meta\"))\n",
    "with open(meta_path, \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "# build label mapping dictionary\n",
    "label_names = meta[\"label_names\"]\n",
    "label_map = {}\n",
    "for label_int, label in enumerate(label_names):\n",
    "    label_map[label_int] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualise image with an interpretable label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'label: horse')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise the image and the corresponding label\n",
    "plt.imshow(rand_img)\n",
    "plt.title(f\"label: {label_map[rand_label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically you explore the data further to see how you can improve your dataset before going further. In the interest of time we will skip that and say:\n",
    "- cifar-10 has no class imbalance issues.\n",
    "- cifar-10 has no missing values.\n",
    "\n",
    "So the hard work has already been completed for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "Now we proceed with preparing the data for ML.\n",
    "\n",
    "But first some background on the representation of the images:   \n",
    "images are represented in the dataset as numpy arrays  with the shape `[height, width, channel]`:\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img alt=\"Image Shape\" width=\"300\" src=\"http://corochann.com/wp-content/uploads/2017/04/cnn_diagram_notation.png\" />\n",
    "    <p>\n",
    "        <b> Channel dimension stores the red, green and blue values </b>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Each 'pixel' in the numpy array ranges from  0 - 255, with 255 as the largest value.\n",
    "\n",
    "### Feature Normalisation\n",
    "First we perform feature normalisation on the input images:\n",
    "- only mean normalisation (subtract mean from image) is required as the pixel come form the same scale (0 - 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature scaling - mean normalisation \n",
    "mean_img = np.mean(train_imgs, axis=0)\n",
    "pp_train_imgs = (train_imgs - mean_img)\n",
    "pp_valid_imgs = (valid_imgs - mean_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Labels\n",
    "As usual, we one hot encode the labels as one hot vectors to enforce the  \n",
    "idea of categorical to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(categories=\"auto\")\n",
    "encoder.fit(train_labels)\n",
    "pp_train_labels = encoder.transform(train_labels).toarray()\n",
    "pp_valid_labels = encoder.transform(valid_labels).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, you do not sample your data, instead you train your model on all the data you have.  \n",
    "In this practical we will artificially limiting the training set to 1/5 of the dataset to observe the effects of too little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute new training set size\n",
    "sample_train_size = len(train_imgs) // 5\n",
    "# sample the trainng example without replacement\n",
    "sample_pp_train_imgs, sample_pp_train_labels = resample(\n",
    "    pp_train_imgs, pp_train_labels, \n",
    "    n_samples=sample_train_size,\n",
    "    replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "Now that data preprocessing has been complete, we can now preceed to build  \n",
    "a model to classify the image. Since this practical focuses on the training  \n",
    "process, we will provide a model generation function for you. \n",
    "\n",
    "Generate the model using `build_model()`: There are some parameters you can configure:\n",
    "- `input_shape` - the shape of the inputs (in this case images) given to the neural network.\n",
    "- `n_output` - no. of outputs in the neural network. Set this to the no. of classes/labels\n",
    "- `scale_width` - increasing this multiples the no. of hidden units used per layer. \n",
    "- `scale_depth` - no. of hidden layers in the network.\n",
    "- `activation` -  the activation function to use in the network. Good choices are Relu, Elu, Selu\n",
    "- `l2_lamda` - amount of L2 regularisation to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzy/.conda/envs/mlbootcamp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# pass the input and no of classes (n_outputs) to build the model\n",
    "input_shape = train_imgs.shape[1:]\n",
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=1, scale_depth=3,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                196672    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 205,642\n",
      "Trainable params: 205,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Now we proceed with training the model. In Keras, there are two steps to   \n",
    "training the model:  \n",
    "1. `.compile()` to compile the model\n",
    "    - Here you will specify the the loss function to minimize,\n",
    "      optimizer that will minimise the loss function (with the corresponding\n",
    "      learning rate) and the metrics to record during training/evaluation  \n",
    "2. `.fit()` to actually train the mode\n",
    "    - train the model by specifying the training data for training,\n",
    "      validation data for metrics during training, the batch size and \n",
    "      no. of epochs/the model passes over the data to train.\n",
    "    - optionally you can specify some usefully callbacks to add\n",
    "      functionality during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compling the model\n",
    "We first compile the model by calling `.compile()` with:\n",
    "- the loss function to minimise:\n",
    "    - classification task\n",
    "        - use `binary_crossentropy` for 2-class/label classification\n",
    "        - use `categorical_crossentropy` for multi-class/label classification\n",
    "    - regression task\n",
    "        - use `mean_squared_error` for regression\n",
    "- the optimizer that will minimise the loss function.\n",
    "    - recommanded choice: `Adam` or `Nadam`\n",
    "    - metrics to evaluate the model during training (ie accuarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "Now we can fit the model to the training data by calling `.fit()`:\n",
    "- provide the training set for the model to train on\n",
    "- provide the validation set for the model to evaluate on\n",
    "- tell the model only many to fit on at once with `batch_size`\n",
    "> If you are training on a hardware accelerator like a GPU, you might want to increase\n",
    "> `batch_size` to take advantage of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzy/.conda/envs/mlbootcamp/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 13.5470 - acc: 0.1556 - val_loss: 13.4733 - val_acc: 0.1627\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 76us/step - loss: 13.4343 - acc: 0.1649 - val_loss: 13.2572 - val_acc: 0.1767\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 13.6097 - acc: 0.1548 - val_loss: 13.6282 - val_acc: 0.1539\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 13.4754 - acc: 0.1635 - val_loss: 13.5978 - val_acc: 0.1562\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 13.5488 - acc: 0.1591 - val_loss: 13.5506 - val_acc: 0.1590\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 73us/step - loss: 13.5506 - acc: 0.1588 - val_loss: 13.6250 - val_acc: 0.1543\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 13.6623 - acc: 0.1519 - val_loss: 13.4931 - val_acc: 0.1627\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 13.5912 - acc: 0.1566 - val_loss: 13.5959 - val_acc: 0.1564\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 13.5775 - acc: 0.1574 - val_loss: 13.6461 - val_acc: 0.1531\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 13.7740 - acc: 0.1453 - val_loss: 13.8791 - val_acc: 0.1388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f054eaa9f28>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model to the data\n",
    "model.fit(sample_pp_train_imgs, sample_pp_train_labels,\n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the metrics looss and accuracy, they seem to be barely improving at all.\n",
    "Lets evaluate the model to investigate what the problem is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We evaluate the model to determine why the model does not train.\n",
    "\n",
    "There is a tool included with `tensorflow` called `tensorboard` that makes it easier to evaluate neural networks  \n",
    "For `tensorboard` to work we have to admend your `.fit()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 77us/step - loss: 13.3534 - acc: 0.1666 - val_loss: 13.1322 - val_acc: 0.1828\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 13.5912 - acc: 0.1556 - val_loss: 13.4534 - val_acc: 0.1642\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 13.4192 - acc: 0.1666 - val_loss: 13.3650 - val_acc: 0.1700\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 13.4634 - acc: 0.1643 - val_loss: 13.4700 - val_acc: 0.1635\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 13.3075 - acc: 0.1738 - val_loss: 13.2751 - val_acc: 0.1760\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 75us/step - loss: 13.3752 - acc: 0.1698 - val_loss: 13.4357 - val_acc: 0.1663\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 77us/step - loss: 13.5495 - acc: 0.1591 - val_loss: 13.4656 - val_acc: 0.1645\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 77us/step - loss: 13.6599 - acc: 0.1522 - val_loss: 14.1533 - val_acc: 0.1216\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 81us/step - loss: 14.1507 - acc: 0.1220 - val_loss: 14.1366 - val_acc: 0.1229\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 81us/step - loss: 14.1977 - acc: 0.1191 - val_loss: 14.1726 - val_acc: 0.1207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f054c4c4a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the input and no of classes (n_outputs) to build the model\n",
    "input_shape = train_imgs.shape[1:]\n",
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=1, scale_depth=3,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=0)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "### New stuff:\n",
    "# we need to name our training run, so we create a name with the \n",
    "# current time\n",
    "run_name = f\"run_{datetime.now():%Y_%m_%d__%H_%M_%S}\"\n",
    "# create directory for storing tensorboard logs\n",
    "logs_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(sample_pp_train_imgs, sample_pp_train_labels,\n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          # set the tensorboard callback to enable tensorboard for the model\n",
    "          callbacks=[callbacks.TensorBoard(log_dir=logs_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training is complete, we have to run the Tensorboard server by:\n",
    "- opening a new terminal in Jupyter Lab (File>New>Terminal)\n",
    "- Running the tensorboard server and pointing the server to the logs with `tensorboard --log-dir=logs`\n",
    "- Point a new tab at http://localhost:6006\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <h5> Tensorboard UI </h5>\n",
    "    <img src=\"https://www.tensorflow.org/images/mnist_tensorboard.png\" width=500/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training loss `loss` we can see that the model having problems fitting the   \n",
    "the training data (its not decreasing much at all).\n",
    "\n",
    "> When this happens the model is having problems **converging** to a lower loss \n",
    "\n",
    "When this happens, its time to tune your learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate. \n",
    "### Learning Rate Tuning\n",
    "Tuning the learning rate is arguablely the important hyperparameter you have to tune when training\n",
    "neural networks:\n",
    "- set the learning rate too high and metrics do not improve or even explode (increase rapidly) during \n",
    "- set the learning rate too low and the neural network takes forever (figure of speech) to train\n",
    "\n",
    "<img alt=\"learning rate tuning\" src=\"https://cdn-images-1.medium.com/max/1200/0*RgLvNta2lZVBtPoc.\" width=300/>\n",
    "\n",
    "We seem to have a high learning rate problem, so we set the learning rate from  \n",
    "the default value of `1e-3` to `1e-4`.\n",
    "\n",
    "> `1e-4` is eqivilent to $1 \\times 10^{-4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 78us/step - loss: 12.2835 - acc: 0.1929 - val_loss: 11.2900 - val_acc: 0.2372\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 10.6626 - acc: 0.2772 - val_loss: 10.3010 - val_acc: 0.2954\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 9.8902 - acc: 0.3199 - val_loss: 10.1081 - val_acc: 0.3066\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 81us/step - loss: 9.4279 - acc: 0.3483 - val_loss: 9.8166 - val_acc: 0.3166\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 83us/step - loss: 8.9481 - acc: 0.3741 - val_loss: 9.5571 - val_acc: 0.3278\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 83us/step - loss: 8.6145 - acc: 0.3949 - val_loss: 9.6439 - val_acc: 0.3157\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 87us/step - loss: 8.3633 - acc: 0.4116 - val_loss: 9.3811 - val_acc: 0.3366\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 81us/step - loss: 8.1298 - acc: 0.4242 - val_loss: 9.2423 - val_acc: 0.3412\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 7.8655 - acc: 0.4365 - val_loss: 9.1471 - val_acc: 0.3456\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 7.7753 - acc: 0.4419 - val_loss: 9.1115 - val_acc: 0.3449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04907ad668>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the input and no of classes (n_outputs) to build the model\n",
    "input_shape = train_imgs.shape[1:]\n",
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=1, scale_depth=3,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=0, dropout_prob=0.0)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(lr=1e-4), # Lowered the learning rate\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# we need to name our training run, so we create a name with the \n",
    "# current time\n",
    "run_name = f\"run_{datetime.now():%Y_%m_%d__%H_%M_%S}\"\n",
    "# create directory for storing tensorboard logs\n",
    "logs_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(sample_pp_train_imgs, sample_pp_train_labels,\n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          # set the tensorboard callback to enable tensorboard for the model\n",
    "          callbacks=[callbacks.TensorBoard(log_dir=logs_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at your tensorboard again. The training loss curve for new training run should appear in tensorboard.\n",
    "> Try different values of the learning rate to see now the learning rate affects convergence.  \n",
    "> Clue: change the power $N$ in `Ae-N` first before tuning the $A$ part of the learning rate  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting \n",
    "A look at the training accuracy shows a dismal picture. The model  \n",
    "appears to be underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a larger model\n",
    "One way to combat underfitting in a neural network is to simply add more hidden layers or more hidden neurons per hidden layer to address overfitting.\n",
    "\n",
    "> With a model building function you can make this very easy by exposing  \n",
    "> it as a function parameter.\n",
    "\n",
    "We both deepen (add more layers) and widen (add more neurons per layer) by  \n",
    "increasing `scale_depth` and `scale_depth` parameters respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 510,218\n",
      "Trainable params: 510,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=2, scale_depth=8,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the larger network to see if it improves our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 3.1940 - acc: 0.1913 - val_loss: 2.1664 - val_acc: 0.2508\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 131us/step - loss: 1.9997 - acc: 0.3042 - val_loss: 2.0380 - val_acc: 0.2843\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 1.8194 - acc: 0.3534 - val_loss: 1.9578 - val_acc: 0.3065\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 133us/step - loss: 1.6940 - acc: 0.4037 - val_loss: 1.9269 - val_acc: 0.3277\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 131us/step - loss: 1.5741 - acc: 0.4474 - val_loss: 1.9176 - val_acc: 0.3302\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 137us/step - loss: 1.4861 - acc: 0.4821 - val_loss: 1.9167 - val_acc: 0.3462\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 129us/step - loss: 1.4014 - acc: 0.5149 - val_loss: 1.9321 - val_acc: 0.3487\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 138us/step - loss: 1.2957 - acc: 0.5524 - val_loss: 1.9470 - val_acc: 0.3508\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 1.2127 - acc: 0.5833 - val_loss: 1.9794 - val_acc: 0.3543\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 1.1214 - acc: 0.6222 - val_loss: 2.0110 - val_acc: 0.3528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0490065f98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(lr=1e-4), \n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# we need to name our training run, so we create a name with the \n",
    "# current time\n",
    "run_name = f\"run_{datetime.now():%Y_%m_%d__%H_%M_%S}\"\n",
    "# create directory for storing tensorboard logs\n",
    "logs_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(sample_pp_train_imgs, sample_pp_train_labels,\n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=10, # increased epochs\n",
    "          # set the tensorboard callback to enable tensorboard for the model\n",
    "          callbacks=[callbacks.TensorBoard(log_dir=logs_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training a larger network, we were attain a higher training accuracy, \n",
    "partially addressing overfitting.\n",
    "\n",
    "#### Tuning the No. of Epochs\n",
    "\n",
    "Looking at the training `loss` learning curve, we can see that training stopped before  \n",
    "the `loss` stopped improving (the training loss has not yet bottomed out).  \n",
    "This is a sign that we have not trained for a suffcient no. of epochs.\n",
    "\n",
    "We train langer by increasing the no. of epoches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "10000/10000 [==============================] - 1s 148us/step - loss: 4.7798 - acc: 0.1926 - val_loss: 3.0961 - val_acc: 0.2260\n",
      "Epoch 2/60\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 2.5068 - acc: 0.2802 - val_loss: 2.5563 - val_acc: 0.2549\n",
      "Epoch 3/60\n",
      "10000/10000 [==============================] - 1s 131us/step - loss: 2.0292 - acc: 0.3439 - val_loss: 2.3802 - val_acc: 0.2713\n",
      "Epoch 4/60\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 1.7777 - acc: 0.4000 - val_loss: 2.3244 - val_acc: 0.2789\n",
      "Epoch 5/60\n",
      "10000/10000 [==============================] - 1s 120us/step - loss: 1.6229 - acc: 0.4449 - val_loss: 2.2764 - val_acc: 0.2890\n",
      "Epoch 6/60\n",
      "10000/10000 [==============================] - 1s 132us/step - loss: 1.4725 - acc: 0.4934 - val_loss: 2.2225 - val_acc: 0.3065\n",
      "Epoch 7/60\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 1.3645 - acc: 0.5324 - val_loss: 2.2371 - val_acc: 0.3071\n",
      "Epoch 8/60\n",
      "10000/10000 [==============================] - 1s 141us/step - loss: 1.2663 - acc: 0.5673 - val_loss: 2.2726 - val_acc: 0.3078\n",
      "Epoch 9/60\n",
      "10000/10000 [==============================] - 1s 126us/step - loss: 1.1593 - acc: 0.6093 - val_loss: 2.2821 - val_acc: 0.3161\n",
      "Epoch 10/60\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 1.0817 - acc: 0.6339 - val_loss: 2.3158 - val_acc: 0.3127\n",
      "Epoch 11/60\n",
      "10000/10000 [==============================] - 1s 126us/step - loss: 1.0093 - acc: 0.6624 - val_loss: 2.3445 - val_acc: 0.3148\n",
      "Epoch 12/60\n",
      "10000/10000 [==============================] - 1s 118us/step - loss: 0.9123 - acc: 0.6994 - val_loss: 2.3909 - val_acc: 0.3312\n",
      "Epoch 13/60\n",
      "10000/10000 [==============================] - 1s 122us/step - loss: 0.8459 - acc: 0.7187 - val_loss: 2.4444 - val_acc: 0.3266\n",
      "Epoch 14/60\n",
      "10000/10000 [==============================] - 1s 121us/step - loss: 0.8157 - acc: 0.7281 - val_loss: 2.4970 - val_acc: 0.3293\n",
      "Epoch 15/60\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 0.7233 - acc: 0.7630 - val_loss: 2.5406 - val_acc: 0.3297\n",
      "Epoch 16/60\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 0.6549 - acc: 0.7947 - val_loss: 2.6114 - val_acc: 0.3313\n",
      "Epoch 17/60\n",
      "10000/10000 [==============================] - 1s 135us/step - loss: 0.5931 - acc: 0.8153 - val_loss: 2.6893 - val_acc: 0.3370\n",
      "Epoch 18/60\n",
      "10000/10000 [==============================] - 1s 132us/step - loss: 0.5503 - acc: 0.8327 - val_loss: 2.7385 - val_acc: 0.3359\n",
      "Epoch 19/60\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 0.5098 - acc: 0.8464 - val_loss: 2.7794 - val_acc: 0.3279\n",
      "Epoch 20/60\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 0.4606 - acc: 0.8638 - val_loss: 2.8563 - val_acc: 0.3347\n",
      "Epoch 21/60\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 0.4081 - acc: 0.8816 - val_loss: 2.9430 - val_acc: 0.3397\n",
      "Epoch 22/60\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 0.3717 - acc: 0.8946 - val_loss: 2.9937 - val_acc: 0.3408\n",
      "Epoch 23/60\n",
      "10000/10000 [==============================] - 1s 129us/step - loss: 0.3385 - acc: 0.9019 - val_loss: 3.1036 - val_acc: 0.3413\n",
      "Epoch 24/60\n",
      "10000/10000 [==============================] - 1s 121us/step - loss: 0.3126 - acc: 0.9125 - val_loss: 3.1835 - val_acc: 0.3452\n",
      "Epoch 25/60\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 0.2855 - acc: 0.9206 - val_loss: 3.2798 - val_acc: 0.3389\n",
      "Epoch 26/60\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 0.2641 - acc: 0.9246 - val_loss: 3.3040 - val_acc: 0.3410\n",
      "Epoch 27/60\n",
      "10000/10000 [==============================] - 1s 138us/step - loss: 0.2377 - acc: 0.9373 - val_loss: 3.4126 - val_acc: 0.3416\n",
      "Epoch 28/60\n",
      "10000/10000 [==============================] - 1s 133us/step - loss: 0.2283 - acc: 0.9390 - val_loss: 3.4892 - val_acc: 0.3373\n",
      "Epoch 29/60\n",
      "10000/10000 [==============================] - 1s 137us/step - loss: 0.1913 - acc: 0.9498 - val_loss: 3.5718 - val_acc: 0.3380\n",
      "Epoch 30/60\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 0.1853 - acc: 0.9521 - val_loss: 3.6824 - val_acc: 0.3370\n",
      "Epoch 31/60\n",
      "10000/10000 [==============================] - 1s 134us/step - loss: 0.1635 - acc: 0.9569 - val_loss: 3.7383 - val_acc: 0.3343\n",
      "Epoch 32/60\n",
      "10000/10000 [==============================] - 1s 134us/step - loss: 0.2193 - acc: 0.9364 - val_loss: 3.8320 - val_acc: 0.3278\n",
      "Epoch 33/60\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 0.2878 - acc: 0.9096 - val_loss: 3.8683 - val_acc: 0.3317\n",
      "Epoch 34/60\n",
      "10000/10000 [==============================] - 1s 133us/step - loss: 0.2438 - acc: 0.9242 - val_loss: 3.8914 - val_acc: 0.3437\n",
      "Epoch 35/60\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 0.1387 - acc: 0.9651 - val_loss: 3.9104 - val_acc: 0.3454\n",
      "Epoch 36/60\n",
      "10000/10000 [==============================] - 1s 131us/step - loss: 0.1053 - acc: 0.9747 - val_loss: 3.9678 - val_acc: 0.3396\n",
      "Epoch 37/60\n",
      "10000/10000 [==============================] - 1s 135us/step - loss: 0.0791 - acc: 0.9843 - val_loss: 4.0902 - val_acc: 0.3412\n",
      "Epoch 38/60\n",
      "10000/10000 [==============================] - 1s 131us/step - loss: 0.0535 - acc: 0.9919 - val_loss: 4.1095 - val_acc: 0.3423\n",
      "Epoch 39/60\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 0.0648 - acc: 0.9866 - val_loss: 4.2224 - val_acc: 0.3432\n",
      "Epoch 40/60\n",
      "10000/10000 [==============================] - 1s 129us/step - loss: 0.1321 - acc: 0.9632 - val_loss: 4.3056 - val_acc: 0.3403\n",
      "Epoch 41/60\n",
      "10000/10000 [==============================] - 1s 134us/step - loss: 0.4229 - acc: 0.8661 - val_loss: 4.2328 - val_acc: 0.3266\n",
      "Epoch 42/60\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 0.3580 - acc: 0.8824 - val_loss: 4.1060 - val_acc: 0.3421\n",
      "Epoch 43/60\n",
      "10000/10000 [==============================] - 1s 132us/step - loss: 0.1650 - acc: 0.9517 - val_loss: 4.2322 - val_acc: 0.3366\n",
      "Epoch 44/60\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 0.0658 - acc: 0.9851 - val_loss: 4.2403 - val_acc: 0.3461\n",
      "Epoch 45/60\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 0.0303 - acc: 0.9967 - val_loss: 4.3544 - val_acc: 0.3452\n",
      "Epoch 46/60\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 0.0190 - acc: 0.9987 - val_loss: 4.4080 - val_acc: 0.3432\n",
      "Epoch 47/60\n",
      "10000/10000 [==============================] - 1s 126us/step - loss: 0.0146 - acc: 0.9993 - val_loss: 4.4578 - val_acc: 0.3480\n",
      "Epoch 48/60\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 0.0112 - acc: 0.9995 - val_loss: 4.5037 - val_acc: 0.3444\n",
      "Epoch 49/60\n",
      "10000/10000 [==============================] - 1s 125us/step - loss: 0.0093 - acc: 0.9995 - val_loss: 4.5474 - val_acc: 0.3516\n",
      "Epoch 50/60\n",
      "10000/10000 [==============================] - 1s 133us/step - loss: 0.0075 - acc: 0.9999 - val_loss: 4.5956 - val_acc: 0.3498\n",
      "Epoch 51/60\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 0.0064 - acc: 0.9999 - val_loss: 4.6243 - val_acc: 0.3518\n",
      "Epoch 52/60\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 4.6679 - val_acc: 0.3541\n",
      "Epoch 53/60\n",
      "10000/10000 [==============================] - 1s 138us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 4.7013 - val_acc: 0.3505\n",
      "Epoch 54/60\n",
      "10000/10000 [==============================] - 1s 132us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 4.7329 - val_acc: 0.3518\n",
      "Epoch 55/60\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 4.7664 - val_acc: 0.3509\n",
      "Epoch 56/60\n",
      "10000/10000 [==============================] - 1s 123us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 4.8081 - val_acc: 0.3539\n",
      "Epoch 57/60\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 4.8425 - val_acc: 0.3530\n",
      "Epoch 58/60\n",
      "10000/10000 [==============================] - 1s 126us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 4.8748 - val_acc: 0.3533\n",
      "Epoch 59/60\n",
      "10000/10000 [==============================] - 1s 126us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 4.9006 - val_acc: 0.3540\n",
      "Epoch 60/60\n",
      "10000/10000 [==============================] - 1s 127us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 4.9407 - val_acc: 0.3513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0392cc4278>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "input_shape = train_imgs.shape[1:]\n",
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=2, scale_depth=6,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=0)\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(lr=1e-4), \n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# we need to name our training run, so we create a name with the \n",
    "# current time\n",
    "run_name = f\"run_{datetime.now():%Y_%m_%d__%H_%M_%S}\"\n",
    "# create directory for storing tensorboard logs\n",
    "logs_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(sample_pp_train_imgs, sample_pp_train_labels,\n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=60, # increased epochs\n",
    "          # set the tensorboard callback to enable tensorboard for the model\n",
    "          callbacks=[callbacks.TensorBoard(log_dir=logs_dir)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training longer we are able to increase our training accuracy tremedously.\n",
    "Consider our underfitting problem solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "Looking at the dismal validation accuracy compared to the training accuracy,  \n",
    "we are overfitting tremendously.\n",
    "\n",
    "#### Regularisation\n",
    "One ways to reduce overfitting is is to introduce regularisation:\n",
    "\n",
    "Types of regularisation you can add:\n",
    "- dropout regularization - randomly _kills_ neurons during training.\n",
    "- l2 regularisation - forces the weights of the model to adapt small value\n",
    "\n",
    "In this practical we will just l2 regularisation to address overfitting \n",
    "by setting `l2_lambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 57.4283 - acc: 0.1938 - val_loss: 53.0112 - val_acc: 0.2212\n",
      "Epoch 2/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 50.7527 - acc: 0.2769 - val_loss: 49.0488 - val_acc: 0.2503\n",
      "Epoch 3/60\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 46.9141 - acc: 0.3440 - val_loss: 45.5151 - val_acc: 0.2814\n",
      "Epoch 4/60\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 43.3628 - acc: 0.4120 - val_loss: 42.1198 - val_acc: 0.3052\n",
      "Epoch 5/60\n",
      "10000/10000 [==============================] - 1s 145us/step - loss: 39.9937 - acc: 0.4609 - val_loss: 38.8570 - val_acc: 0.3232\n",
      "Epoch 6/60\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 36.7650 - acc: 0.5098 - val_loss: 35.7529 - val_acc: 0.3272\n",
      "Epoch 7/60\n",
      "10000/10000 [==============================] - 1s 146us/step - loss: 33.6985 - acc: 0.5344 - val_loss: 32.7804 - val_acc: 0.3478\n",
      "Epoch 8/60\n",
      "10000/10000 [==============================] - 2s 153us/step - loss: 30.8104 - acc: 0.5563 - val_loss: 29.9821 - val_acc: 0.3601\n",
      "Epoch 9/60\n",
      "10000/10000 [==============================] - 1s 146us/step - loss: 28.0857 - acc: 0.5792 - val_loss: 27.3438 - val_acc: 0.3695\n",
      "Epoch 10/60\n",
      "10000/10000 [==============================] - 2s 158us/step - loss: 25.5500 - acc: 0.5844 - val_loss: 24.8871 - val_acc: 0.3762\n",
      "Epoch 11/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 23.1784 - acc: 0.5991 - val_loss: 22.5885 - val_acc: 0.3932\n",
      "Epoch 12/60\n",
      "10000/10000 [==============================] - 2s 153us/step - loss: 20.9858 - acc: 0.6017 - val_loss: 20.4837 - val_acc: 0.4011\n",
      "Epoch 13/60\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 18.9696 - acc: 0.6069 - val_loss: 18.5287 - val_acc: 0.4090\n",
      "Epoch 14/60\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 17.1152 - acc: 0.6025 - val_loss: 16.7606 - val_acc: 0.4104\n",
      "Epoch 15/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 15.4176 - acc: 0.6132 - val_loss: 15.1273 - val_acc: 0.4221\n",
      "Epoch 16/60\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 13.8871 - acc: 0.6125 - val_loss: 13.6691 - val_acc: 0.4306\n",
      "Epoch 17/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 12.5001 - acc: 0.6150 - val_loss: 12.3369 - val_acc: 0.4278\n",
      "Epoch 18/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 11.2456 - acc: 0.6197 - val_loss: 11.1576 - val_acc: 0.4376\n",
      "Epoch 19/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 10.1207 - acc: 0.6306 - val_loss: 10.1335 - val_acc: 0.4332\n",
      "Epoch 20/60\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 9.1383 - acc: 0.6278 - val_loss: 9.2118 - val_acc: 0.4331\n",
      "Epoch 21/60\n",
      "10000/10000 [==============================] - 2s 153us/step - loss: 8.2531 - acc: 0.6327 - val_loss: 8.4056 - val_acc: 0.4299\n",
      "Epoch 22/60\n",
      "10000/10000 [==============================] - 2s 153us/step - loss: 7.4869 - acc: 0.6327 - val_loss: 7.6633 - val_acc: 0.4444\n",
      "Epoch 23/60\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 6.7991 - acc: 0.6376 - val_loss: 7.0457 - val_acc: 0.4408\n",
      "Epoch 24/60\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 6.2062 - acc: 0.6436 - val_loss: 6.4872 - val_acc: 0.4417\n",
      "Epoch 25/60\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 5.6848 - acc: 0.6460 - val_loss: 5.9854 - val_acc: 0.4503\n",
      "Epoch 26/60\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 5.2221 - acc: 0.6509 - val_loss: 5.5943 - val_acc: 0.4389\n",
      "Epoch 27/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 4.8147 - acc: 0.6636 - val_loss: 5.2335 - val_acc: 0.4492\n",
      "Epoch 28/60\n",
      "10000/10000 [==============================] - 2s 153us/step - loss: 4.4905 - acc: 0.6596 - val_loss: 4.9060 - val_acc: 0.4455\n",
      "Epoch 29/60\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 4.1887 - acc: 0.6612 - val_loss: 4.6319 - val_acc: 0.4459\n",
      "Epoch 30/60\n",
      "10000/10000 [==============================] - 1s 147us/step - loss: 3.9059 - acc: 0.6672 - val_loss: 4.4408 - val_acc: 0.4373\n",
      "Epoch 31/60\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 3.6782 - acc: 0.6694 - val_loss: 4.2201 - val_acc: 0.4309\n",
      "Epoch 32/60\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 3.4666 - acc: 0.6784 - val_loss: 4.0311 - val_acc: 0.4399\n",
      "Epoch 33/60\n",
      "10000/10000 [==============================] - 2s 158us/step - loss: 3.2990 - acc: 0.6772 - val_loss: 3.8934 - val_acc: 0.4346\n",
      "Epoch 34/60\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 3.1512 - acc: 0.6727 - val_loss: 3.7205 - val_acc: 0.4456\n",
      "Epoch 35/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 3.0050 - acc: 0.6767 - val_loss: 3.5698 - val_acc: 0.4456\n",
      "Epoch 36/60\n",
      "10000/10000 [==============================] - 2s 174us/step - loss: 2.8698 - acc: 0.6842 - val_loss: 3.5034 - val_acc: 0.4445\n",
      "Epoch 37/60\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 2.7667 - acc: 0.6837 - val_loss: 3.4355 - val_acc: 0.4279\n",
      "Epoch 38/60\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 2.6776 - acc: 0.6831 - val_loss: 3.3114 - val_acc: 0.4416\n",
      "Epoch 39/60\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 2.5843 - acc: 0.6854 - val_loss: 3.2489 - val_acc: 0.4436\n",
      "Epoch 40/60\n",
      "10000/10000 [==============================] - 1s 150us/step - loss: 2.5180 - acc: 0.6865 - val_loss: 3.2086 - val_acc: 0.4408\n",
      "Epoch 41/60\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 2.4288 - acc: 0.7001 - val_loss: 3.1503 - val_acc: 0.4418\n",
      "Epoch 42/60\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 2.3870 - acc: 0.6927 - val_loss: 3.0766 - val_acc: 0.4430\n",
      "Epoch 43/60\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 2.3333 - acc: 0.6930 - val_loss: 3.0384 - val_acc: 0.4377\n",
      "Epoch 44/60\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 2.2725 - acc: 0.7027 - val_loss: 3.0440 - val_acc: 0.4254\n",
      "Epoch 45/60\n",
      "10000/10000 [==============================] - 2s 158us/step - loss: 2.2347 - acc: 0.7004 - val_loss: 2.9582 - val_acc: 0.4447\n",
      "Epoch 46/60\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 2.1777 - acc: 0.7091 - val_loss: 3.0042 - val_acc: 0.4358\n",
      "Epoch 47/60\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 2.1450 - acc: 0.7076 - val_loss: 2.8773 - val_acc: 0.4502\n",
      "Epoch 48/60\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 2.1076 - acc: 0.7072 - val_loss: 2.9190 - val_acc: 0.4282\n",
      "Epoch 49/60\n",
      "10000/10000 [==============================] - 2s 158us/step - loss: 2.0788 - acc: 0.7090 - val_loss: 2.9470 - val_acc: 0.4293\n",
      "Epoch 50/60\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 2.0458 - acc: 0.7157 - val_loss: 2.8877 - val_acc: 0.4431\n",
      "Epoch 51/60\n",
      "10000/10000 [==============================] - 1s 148us/step - loss: 2.0252 - acc: 0.7129 - val_loss: 2.8301 - val_acc: 0.4465\n",
      "Epoch 52/60\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 1.9815 - acc: 0.7284 - val_loss: 2.8577 - val_acc: 0.4400\n",
      "Epoch 53/60\n",
      "10000/10000 [==============================] - 2s 156us/step - loss: 1.9779 - acc: 0.7231 - val_loss: 2.8455 - val_acc: 0.4382\n",
      "Epoch 54/60\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 1.9542 - acc: 0.7263 - val_loss: 2.8407 - val_acc: 0.4280\n",
      "Epoch 55/60\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 1.9370 - acc: 0.7215 - val_loss: 2.8813 - val_acc: 0.4197\n",
      "Epoch 56/60\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 1.9122 - acc: 0.7280 - val_loss: 2.8115 - val_acc: 0.4317\n",
      "Epoch 57/60\n",
      "10000/10000 [==============================] - 1s 149us/step - loss: 1.8787 - acc: 0.7392 - val_loss: 2.8133 - val_acc: 0.4463\n",
      "Epoch 58/60\n",
      "10000/10000 [==============================] - 2s 158us/step - loss: 1.8535 - acc: 0.7430 - val_loss: 2.8387 - val_acc: 0.4381\n",
      "Epoch 59/60\n",
      "10000/10000 [==============================] - 2s 154us/step - loss: 1.8558 - acc: 0.7399 - val_loss: 2.8269 - val_acc: 0.4381\n",
      "Epoch 60/60\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 1.8441 - acc: 0.7409 - val_loss: 2.8205 - val_acc: 0.4296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0390901208>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "input_shape = train_imgs.shape[1:]\n",
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=2, scale_depth=6,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=6e-2) # added l2 regularisatin\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(lr=1e-4), \n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# we need to name our training run, so we create a name with the \n",
    "# current time\n",
    "run_name = f\"run_{datetime.now():%Y_%m_%d__%H_%M_%S}\"\n",
    "# create directory for storing tensorboard logs\n",
    "logs_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(sample_pp_train_imgs, sample_pp_train_labels,\n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=60, \n",
    "          verbose=1,\n",
    "          # set the tensorboard callback to enable tensorboard for the model\n",
    "          callbacks=[callbacks.TensorBoard(log_dir=logs_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Adding l2 regularisation will increase your training and validation \n",
    "> losses compared to without l2. So please do not be alarmed.\n",
    "\n",
    "##### Bias Variance Tradeoff\n",
    "Regularisation was solved our overfitting problem but has also caused our model's  \n",
    "training accuracy to drop significantly. This is because regularisation is is no   \n",
    "magic bullet. Its simply a trade of overfitting (variance) with underfitting(bias).\n",
    "\n",
    "> Argueablely, this is the best (dense only) neural network we can build with  \n",
    "> this small 1/5 sample of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Data\n",
    "Another way to address overfitting to to simply to train with more data.\n",
    "> Neural Networks excell at fitting large datasets.\n",
    "\n",
    "Remember how we sampled the dataset earlier to reduce it to 1/5 of its actual size?  \n",
    "To improve overfitting we can try retraining with the original size of the training set.\n",
    "\n",
    "> We won't be doing it in the practical because it takes quite some time to train\n",
    "> Heres the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 11.4329 - acc: 0.2517 - val_loss: 9.8500 - val_acc: 0.3130\n",
      "Epoch 2/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 9.1769 - acc: 0.3590 - val_loss: 8.5998 - val_acc: 0.3839\n",
      "Epoch 3/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 7.9921 - acc: 0.4176 - val_loss: 7.4945 - val_acc: 0.4195\n",
      "Epoch 4/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 6.9115 - acc: 0.4544 - val_loss: 6.4717 - val_acc: 0.4384\n",
      "Epoch 5/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 5.9296 - acc: 0.4801 - val_loss: 5.5628 - val_acc: 0.4596\n",
      "Epoch 6/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 5.0738 - acc: 0.5037 - val_loss: 4.7941 - val_acc: 0.4708\n",
      "Epoch 7/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 4.3654 - acc: 0.5188 - val_loss: 4.1732 - val_acc: 0.4841\n",
      "Epoch 8/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 3.7926 - acc: 0.5346 - val_loss: 3.6820 - val_acc: 0.4873\n",
      "Epoch 9/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 3.3479 - acc: 0.5463 - val_loss: 3.3121 - val_acc: 0.4962\n",
      "Epoch 10/60\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 3.0002 - acc: 0.5555 - val_loss: 3.0227 - val_acc: 0.5013\n",
      "Epoch 11/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 2.7381 - acc: 0.5640 - val_loss: 2.8030 - val_acc: 0.5054\n",
      "Epoch 12/60\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 2.5280 - acc: 0.5752 - val_loss: 2.6484 - val_acc: 0.5061\n",
      "Epoch 13/60\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 2.3657 - acc: 0.5822 - val_loss: 2.4918 - val_acc: 0.5140\n",
      "Epoch 14/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 2.2315 - acc: 0.5898 - val_loss: 2.4062 - val_acc: 0.5123\n",
      "Epoch 15/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 2.1217 - acc: 0.5942 - val_loss: 2.2984 - val_acc: 0.5225\n",
      "Epoch 16/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 2.0297 - acc: 0.6021 - val_loss: 2.2392 - val_acc: 0.5225\n",
      "Epoch 17/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.9539 - acc: 0.6090 - val_loss: 2.1738 - val_acc: 0.5181\n",
      "Epoch 18/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.8853 - acc: 0.6117 - val_loss: 2.1176 - val_acc: 0.5241\n",
      "Epoch 19/60\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 1.8285 - acc: 0.6211 - val_loss: 2.0679 - val_acc: 0.5289\n",
      "Epoch 20/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 1.7809 - acc: 0.6200 - val_loss: 2.0626 - val_acc: 0.5216\n",
      "Epoch 21/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 1.7329 - acc: 0.6286 - val_loss: 2.0279 - val_acc: 0.5253\n",
      "Epoch 22/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.6929 - acc: 0.6322 - val_loss: 1.9809 - val_acc: 0.5274\n",
      "Epoch 23/60\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 1.6559 - acc: 0.6371 - val_loss: 1.9826 - val_acc: 0.5289\n",
      "Epoch 24/60\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 1.6255 - acc: 0.6389 - val_loss: 1.9672 - val_acc: 0.5255\n",
      "Epoch 25/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.5904 - acc: 0.6464 - val_loss: 1.9383 - val_acc: 0.5329\n",
      "Epoch 26/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.5687 - acc: 0.6506 - val_loss: 1.9378 - val_acc: 0.5255\n",
      "Epoch 27/60\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 1.5446 - acc: 0.6514 - val_loss: 1.9223 - val_acc: 0.5341\n",
      "Epoch 28/60\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 1.5201 - acc: 0.6559 - val_loss: 1.9260 - val_acc: 0.5250\n",
      "Epoch 29/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 1.4991 - acc: 0.6611 - val_loss: 1.9087 - val_acc: 0.5330\n",
      "Epoch 30/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 1.4770 - acc: 0.6655 - val_loss: 1.8916 - val_acc: 0.5284\n",
      "Epoch 31/60\n",
      "50000/50000 [==============================] - 3s 67us/step - loss: 1.4561 - acc: 0.6693 - val_loss: 1.9051 - val_acc: 0.5303\n",
      "Epoch 32/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 1.4443 - acc: 0.6710 - val_loss: 1.8956 - val_acc: 0.5308\n",
      "Epoch 33/60\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 1.4318 - acc: 0.6707 - val_loss: 1.8938 - val_acc: 0.5268\n",
      "Epoch 34/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.4144 - acc: 0.6765 - val_loss: 1.8766 - val_acc: 0.5337\n",
      "Epoch 35/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.4049 - acc: 0.6762 - val_loss: 1.8537 - val_acc: 0.5397\n",
      "Epoch 36/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.3854 - acc: 0.6813 - val_loss: 1.8905 - val_acc: 0.5261\n",
      "Epoch 37/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.3744 - acc: 0.6835 - val_loss: 1.9152 - val_acc: 0.5244\n",
      "Epoch 38/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.3673 - acc: 0.6853 - val_loss: 1.8801 - val_acc: 0.5282\n",
      "Epoch 39/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 1.3576 - acc: 0.6865 - val_loss: 1.8604 - val_acc: 0.5351\n",
      "Epoch 40/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.3455 - acc: 0.6917 - val_loss: 1.8809 - val_acc: 0.5298\n",
      "Epoch 41/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.3305 - acc: 0.6970 - val_loss: 1.8705 - val_acc: 0.5381\n",
      "Epoch 42/60\n",
      "50000/50000 [==============================] - 3s 70us/step - loss: 1.3285 - acc: 0.6960 - val_loss: 1.8784 - val_acc: 0.5340\n",
      "Epoch 43/60\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 1.3138 - acc: 0.7015 - val_loss: 1.8947 - val_acc: 0.5309\n",
      "Epoch 44/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.3104 - acc: 0.6993 - val_loss: 1.8912 - val_acc: 0.5310\n",
      "Epoch 45/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.3015 - acc: 0.7047 - val_loss: 1.8970 - val_acc: 0.5292\n",
      "Epoch 46/60\n",
      "50000/50000 [==============================] - 4s 70us/step - loss: 1.2914 - acc: 0.7058 - val_loss: 1.8778 - val_acc: 0.5380\n",
      "Epoch 47/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 1.2876 - acc: 0.7077 - val_loss: 1.8951 - val_acc: 0.5299\n",
      "Epoch 48/60\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.2748 - acc: 0.7134 - val_loss: 1.9027 - val_acc: 0.5361\n",
      "Epoch 49/60\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 1.2717 - acc: 0.7131 - val_loss: 1.9186 - val_acc: 0.5230\n",
      "Epoch 50/60\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 1.2637 - acc: 0.7159 - val_loss: 1.9079 - val_acc: 0.5288\n",
      "Epoch 51/60\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 1.2585 - acc: 0.7178 - val_loss: 1.9306 - val_acc: 0.5300\n",
      "Epoch 52/60\n",
      "50000/50000 [==============================] - 4s 72us/step - loss: 1.2500 - acc: 0.7180 - val_loss: 1.9292 - val_acc: 0.5291\n",
      "Epoch 53/60\n",
      "50000/50000 [==============================] - 4s 73us/step - loss: 1.2422 - acc: 0.7217 - val_loss: 1.9160 - val_acc: 0.5369\n",
      "Epoch 54/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.2410 - acc: 0.7216 - val_loss: 1.9501 - val_acc: 0.5247\n",
      "Epoch 55/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.2320 - acc: 0.7252 - val_loss: 1.9533 - val_acc: 0.5231\n",
      "Epoch 56/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.2319 - acc: 0.7235 - val_loss: 1.9028 - val_acc: 0.5300\n",
      "Epoch 57/60\n",
      "50000/50000 [==============================] - 3s 69us/step - loss: 1.2256 - acc: 0.7268 - val_loss: 1.9286 - val_acc: 0.5298\n",
      "Epoch 58/60\n",
      "50000/50000 [==============================] - 3s 68us/step - loss: 1.2145 - acc: 0.7333 - val_loss: 1.9566 - val_acc: 0.5314\n",
      "Epoch 59/60\n",
      "50000/50000 [==============================] - 4s 71us/step - loss: 1.2079 - acc: 0.7329 - val_loss: 1.9628 - val_acc: 0.5239\n",
      "Epoch 60/60\n",
      "50000/50000 [==============================] - 4s 74us/step - loss: 1.2104 - acc: 0.7340 - val_loss: 1.9576 - val_acc: 0.5199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ba05ec940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "input_shape = train_imgs.shape[1:]\n",
    "model = baseline_model.build_model(\n",
    "    input_shape, n_outputs=10, scale_width=2, scale_depth=6,\n",
    "    activation=layers.ReLU,\n",
    "    l2_lambda=1e-2) # admend l2 regularisation\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizers.Adam(lr=1e-4), \n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# we need to name our training run, so we create a name with the \n",
    "# current time\n",
    "run_name = f\"run_{datetime.now():%Y_%m_%d__%H_%M_%S}\"\n",
    "# create directory for storing tensorboard logs\n",
    "logs_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(pp_train_imgs, pp_train_labels, # train with entire actual training set \n",
    "          validation_data=(pp_valid_imgs, pp_valid_labels),\n",
    "          batch_size=64,\n",
    "          epochs=60, \n",
    "          verbose=1,\n",
    "          # set the tensorboard callback to enable tensorboard for the model\n",
    "          callbacks=[callbacks.TensorBoard(log_dir=logs_dir)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbootcamp-py",
   "language": "python",
   "name": "mlbootcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
